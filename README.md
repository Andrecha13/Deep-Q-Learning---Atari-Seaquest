ğŸ“˜ Proyecto: Deep Q-Learning para Atari Seaquest

Autor: AndrÃ© ChÃ¡vez Contreras
Universidad de Xalapa â€” IngenierÃ­a en Inteligencia Artificial

ğŸ  DescripciÃ³n del Proyecto

Este repositorio contiene el desarrollo de un agente de Deep Q-Learning (DQN) entrenado para jugar Seaquest, un videojuego de Atari incluido en los entornos de Gymnasium.
El entrenamiento se realizÃ³ utilizando Stable-Baselines3, siguiendo buenas prÃ¡cticas como:

Preprocesamiento estÃ¡ndar para Atari (84Ã—84, escala de grises, frame stacking).

Entrenamiento con entornos vectorizados (n_envs=4).

Versiones de las librerias:
Python 3.12.7
gymnasium 1.2.2
ale-py 0.9.0
stable-baselines3 2.7.0
torch 2.9.1+cu130
opencv-python 4.10.0.84


Uso de experience replay, redes objetivo y exploraciÃ³n epsilon-greedy.

ContinuaciÃ³n del entrenamiento desde checkpoints.

EvaluaciÃ³n determinista del desempeÃ±o final del agente.

Este trabajo forma parte del reporte acadÃ©mico en formato IEEE correspondiente a la segunda unidad.

ğŸ“‚ Contenido del Repositorio
ğŸ“ Proyecto-DQL-Seaquest
â”‚
â”œâ”€â”€ modelos/
â”‚   â””â”€â”€ dqn_seaquest_run9_finished_11429248_steps.zip                 # Ãšltimo modelo entrenado
â”‚
â”œâ”€â”€ video/
â”‚   â””â”€â”€ seaquest_30s_demo-episode-0.mp4              # Video de evaluaciÃ³n del agente
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ Entrenamiento_inicial.py         # Fase I: entrenamiento base
â”‚   â”œâ”€â”€ ContinuarEntrenamiento.py        # Fase II: continuar + optimizar
â”‚   â””â”€â”€ Visualizar_modelo.py             # Fase III: evaluaciÃ³n / videos
â”‚
â”œâ”€â”€ reporte/
â”‚   â””â”€â”€ Proyecto_Agentes_p2.pdf                 # Documento oficial del proyecto
â”‚
â””â”€â”€ README.md                            # ğŸ“˜ Este archivo

ğŸš€ CÃ³mo Ejecutar el Proyecto
1ï¸âƒ£ Instalar dependencias
pip install \
  gymnasium==1.2.2 \
  gymnasium[atari]==1.2.2 \
  gymnasium[accept-rom-license]==1.2.2 \
  ale-py==0.9.0 \
  stable-baselines3==2.7.0 \
  torch==2.9.1+cu130 \
  torchvision==0.20.1+cu130 \
  torchaudio==2.9.1+cu130 \
  opencv-python==4.10.0.84

pip install typing-extensions


2ï¸âƒ£ Entrenar desde cero
python scripts/Entrenamiento_inicial.py

3ï¸âƒ£ Continuar el entrenamiento
python scripts/ContinuarEntrenamiento.py

4ï¸âƒ£ Visualizar el agente
python scripts/Visualizar_modelo.py

ğŸ“Š Resultados principales

El modelo final entrenado alcanzÃ³:

â‰ˆ 11.4M timesteps totales

Recompensa promedio (greedy): ~1152 puntos

Aprendizaje de comportamientos clave:
âœ“ Eliminar enemigos
âœ“ Rescatar buzos
âœ“ Subir para recargar oxÃ­geno

ğŸ”§ Trabajo Futuro

Probar variantes avanzadas de DQN: Double DQN, PER, Dueling DQN, NoisyNets.

Comparar con algoritmos PPO, A2C o Rainbow.

AÃ±adir mÃ©tricas mÃ¡s completas y anÃ¡lisis de curvas de aprendizaje.

Mejorar reproducibilidad y limpieza del repositorio.

ğŸ“š Referencias

Stable Baselines 3: https://stable-baselines3.readthedocs.io

Gymnasium Atari: https://gymnasium.farama.org/environments/atari/

RL Baselines3 Zoo: https://github.com/DLR-RM/rl-baselines3-zoo

Deep RL Course (HuggingFace): https://huggingface.co/learn/deep-rl

Mnih et al., â€œHuman-level control through deep reinforcement learningâ€ (Nature, 2015)